{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Making Your First TensorFlow Experience a Pleasure","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://user-images.githubusercontent.com/115424463/267539825-8e845684-c60a-4bc9-838d-f0b800e317b9.jpeg\" alt=\"'Petals to the Metal' by AI\">\n\n### Picture: 'Petals to the Metal' by AI","metadata":{}},{"cell_type":"markdown","source":"# Intro #\n\nThis notebook builds on [**Create Your First Sumbission**](https://www.kaggle.com/code/ryanholbrook/create-your-first-submission) notebook and aims to facilitate the transition from the micro-course [**Computer Vision**](https://www.kaggle.com/learn/computer-vision) to the first [**Petals to the Metal**](https://www.kaggle.com/c/tpu-getting-started) competition (image classification).\n\nFirst and foremost, this is the perfect opportunity to express immense gratitude to the authors of the **Computer Vision** course and **Create Your First Submission** notebook. It's truly miraculous that materials enabling newcomers to progress from ground zero to actually reading a TensorFlow code in no time are available for free!\n\nThe micro-course is completely self-contained and doesn't require additional reading for progress. At the same time, it took me a while to comprehend the code in the \"Create Your First Submission\" notebook. So, I thought there might be some value in bridging that gap with additional explanations and simplifications where possible, while ensuring that the \"first experience\" remains concise but adequate for submitting to the competition. \n\nI also added some insights from [**George Zoto's** notebook](https://www.kaggle.com/code/georgezoto/computer-vision-petals-to-the-metal) to make the script produce a high-perfoming model (check the link to see a thorough and inspiring project!)\n\n<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n    <strong> Fork This Notebook </strong> by clicking on the <strong> Copy and Edit button</strong> in the top right corner.\n    The notebook is designed to improve your visual comprehension, but you can only see this structure when you're editing it. <br> </blockquote>","metadata":{}},{"cell_type":"markdown","source":"# Step 0: Imports","metadata":{}},{"cell_type":"code","source":"# !pip install --upgrade pip\n# !pip install -U tensorflow == 2.11.0  \n\nimport math, re, os\nimport numpy as np\nimport tensorflow as tf\nprint(f'TensorFlow version: {tf.__version__}')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:36.263583Z","iopub.execute_input":"2023-09-13T08:34:36.264691Z","iopub.status.idle":"2023-09-13T08:34:36.271018Z","shell.execute_reply.started":"2023-09-13T08:34:36.264626Z","shell.execute_reply":"2023-09-13T08:34:36.269822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Connect to Tensor Processing Units (TPUs)\n\nKaggle provides a limited access to 3 types of processing units, avaliale for your models' training.\n- Central Processing Units (**CPUs**)\n- Graphics Processing Units (**GPUs**)\n- Tensor Processing Units (**TPUs**)\n\nHere is an [**article**](https://towardsdatascience.com/when-to-use-cpus-vs-gpus-vs-tpus-in-a-kaggle-competition-9af708a8c3eb) to help you figure out which is which. Long story short, TPU is hardware, specifically created to train TensorFlow models.\n\nA TPU has **eight cores** (it's like having eight GPUs in one machine). \nWith **distribution strategy**, we instruct TensorFlow on how to utilize all these cores simultaneously. We will employ this object when constructing our neural network model: it will distribute the training by generating eight distinct *replicas* of the model, one for each core.","metadata":{}},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # An attempt to detect an avaliable TPU (to 'resolve a TPU cluster')\n    print('Running on TPU ', tpu.master())                     # Tell the world that the attempt was a success!\nexcept ValueError:\n    tpu = None\n\nif tpu:                                                        # If TPU was detected\n    tf.config.experimental_connect_to_cluster(tpu)             # connect to the TPU cluster \n    tf.tpu.experimental.initialize_tpu_system(tpu)             # run the cluster\n    strategy = tf.distribute.TPUStrategy(tpu)                  # create a distribution strategy for TPU training\nelse:\n    strategy = tf.distribute.get_strategy()                    # If no TPU was found, use the default distribution strategy for CPU or GPU\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)             # Tell the world which strategy it is","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:41.890466Z","iopub.execute_input":"2023-09-13T08:34:41.891382Z","iopub.status.idle":"2023-09-13T08:34:41.911088Z","shell.execute_reply.started":"2023-09-13T08:34:41.891336Z","shell.execute_reply":"2023-09-13T08:34:41.910030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set how many files can be processed simultaniously. This will be 16 with TPU off and 128 (=16*8) with TPU on\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:42.088078Z","iopub.execute_input":"2023-09-13T08:34:42.088390Z","iopub.status.idle":"2023-09-13T08:34:42.092696Z","shell.execute_reply.started":"2023-09-13T08:34:42.088365Z","shell.execute_reply":"2023-09-13T08:34:42.091554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Retrieve, Load and Format Data\n\n- When used with TPUs, datasets need to be stored in a [Google Cloud Storage bucket](https://cloud.google.com/storage/) (**GCS**). You can use data from any public GCS bucket by giving its path (like `'/kaggle/input'`). \n- You can use data from any public dataset here on Kaggle in just the same way. If you'd like to use data from one of your private datasets, see [here](https://www.kaggle.com/docs/tpu#tpu3pt5).\n- When used with TPUs, datasets are serialized into [TFRecords](https://www.kaggle.com/ryanholbrook/tfrecords-basics). This is a format for distributing data to each of the TPUs cores.","metadata":{}},{"cell_type":"code","source":"# Here we create lists of paths to our training, validation and test files\nfrom kaggle_datasets import KaggleDatasets\n\nGCS_DS_PATH     = KaggleDatasets().get_gcs_path('tpu-getting-started')   # You can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\nGCS_DS_PATH_EXT = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec') # More data from a side source! If you get an error here, add 'tf-flower-photo-tfrec' in the 'Add data' tab\n\nIMAGE_SIZE = [192, 192]                                                  #this is the size for GPU. For TPU use [512, 512]\n                 \nGCS_PATH_SELECT = {                                                      # Images of different sizes are strored in different directories. The dictionary connects the sizes to the paths\n    192: '/tfrecords-jpeg-192x192',\n    224: '/tfrecords-jpeg-224x224',\n    331: '/tfrecords-jpeg-331x331',\n    512: '/tfrecords-jpeg-512x512'\n}\n\nGCS_PATH_PER_SIZE = GCS_PATH_SELECT[IMAGE_SIZE[0]]                       # Define the path to the directory depending on the IMAGE_SIZE\nGCS_PATH_ORIGINAL = GCS_DS_PATH + GCS_PATH_PER_SIZE                      # This is where the original data for the competition dwells\n\nIMAGENET_FILES    = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/imagenet'    + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\nINATURELIST_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/inaturalist' + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\nOPENIMAGE_FILES   = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/openimage'   + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\nOXFORD_FILES      = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/oxford_102'  + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\nTENSORFLOW_FILES  = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/tf_flowers'  + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\n\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH_ORIGINAL  + '/train/*.tfrec')  # Get the list of file paths for training TFRecords\nTRAINING_FILENAMES = TRAINING_FILENAMES + IMAGENET_FILES + INATURELIST_FILES + OPENIMAGE_FILES + OXFORD_FILES + TENSORFLOW_FILES  # Add the extra data\n\n\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH_ORIGINAL + '/val/*.tfrec')   # Get the list of file paths for validation TFRecords\nTEST_FILENAMES       = tf.io.gfile.glob(GCS_PATH_ORIGINAL + '/test/*.tfrec')  # Get the list of file paths for testing TFRecords","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:42.531656Z","iopub.execute_input":"2023-09-13T08:34:42.532538Z","iopub.status.idle":"2023-09-13T08:34:43.731763Z","shell.execute_reply.started":"2023-09-13T08:34:42.532489Z","shell.execute_reply":"2023-09-13T08:34:43.730976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These are our classification labels \nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']      ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:43.734730Z","iopub.execute_input":"2023-09-13T08:34:43.735328Z","iopub.status.idle":"2023-09-13T08:34:43.756896Z","shell.execute_reply.started":"2023-09-13T08:34:43.735284Z","shell.execute_reply":"2023-09-13T08:34:43.751577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE                     # Configure Auto-tuning for better performance. To be applied in many functions below                                                                                                                                # 100 - 102\n\ndef decode_image(image_data):                            \n    image = tf.image.decode_jpeg(image_data, channels=3) # Decode the JPEG image to a tensor with 3 color channels (red, green, blue)\n    image = tf.cast(image, tf.float32) / 255.0           # Convert pixel values to floating-point numbers in the range [0, 1]\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])          # Reshape the image tensor to match the specified IMAGE_SIZE\n                                                         # This step ensures that all images have the same dimensions for consistency\n    return image\n\n# This function reads a labeled TFRecord file and returns the image and its corresponding label (to be applied on training and validation sets)\ndef read_labeled_tfrecord(example):                                     # example: A single labeled TFRecord file (labled picture to be used for training and validation)\n    LABELED_TFREC_FORMAT = {                                            # setting a dictionary that defines the format of a TFRecord (names and dtypes of its features)\n        \"image\": tf.io.FixedLenFeature([], tf.string),                  # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),                   # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) # Parse the single TFRecord example according to the specified format.\n    image = decode_image(example['image'])                              # Decode the 'image' feature of a TFRecord file using the 'decode_image' function (previously defined)                         \n    label = tf.cast(example['class'], tf.int32)                         # tf.cast converts tensors from one data type to another. Here it ensures that all elements are integers\n    return image, label                                                 # returns a dataset of (image, label) pairs. In Python you get a tuple with this syntaxis automatically\n\n\n# This function reads an unlabeled TFRecord file and returns the image and its ID (to be applied on the test set)\ndef read_unlabeled_tfrecord(example):                       \n    UNLABELED_TFREC_FORMAT = {                             \n        \"image\": tf.io.FixedLenFeature([], tf.string),     \n        \"id\": tf.io.FixedLenFeature([], tf.string),                       # Class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT) # Parse the single TFRecord example according to the specified format.\n    image = decode_image(example['image'])                                # Decode the 'image' feature of a TFRecord example using the 'decode_image' function (previously defined)\n    idnum = example['id']\n    return image, idnum                                                   # Returns a dataset of (image, id) pairs. In Python you get a tuple with this syntaxis automatically\n\n\n# Read from TFRecords. For optimal performance, reading from multiple files at once and disregarding data order.\ndef load_dataset(filenames, labeled=True, ordered=False):                 # We set values for 'labeled' and 'ordered' in the definition of the function to use them by default. However, we reserve an option to pass different values to these parameters.\n    \n    options = tf.data.Options()                                           # Creating an objects here looks like a TensorFlow reference code. It is literally the same as in the documentation\n    if not ordered:                                                       # If the 'ordered' parameter is 'False' (the default) and hasn't been explicitly set to 'True' when passed to the function\n        options.deterministic = False                                     # Disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files. This 'num_parallel_reads=AUTO' parameter of tf.data.TFRecordDataset() will be used many times in the below code.\n    dataset = dataset.with_options(options)                               # uses data as soon as it streams in, rather than in its original order\n                          \n                          # returns a dataset of (image, label) pairs if labeled=True\n    dataset = dataset.map(read_labeled_tfrecord if labeled \\\n                          # returns a dataset of (image, id) pairs if labeled=False\n                          else read_unlabeled_tfrecord,\n                          num_parallel_calls=AUTO)                        \n    return dataset","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T08:34:43.761481Z","iopub.execute_input":"2023-09-13T08:34:43.767940Z","iopub.status.idle":"2023-09-13T08:34:43.786570Z","shell.execute_reply.started":"2023-09-13T08:34:43.767879Z","shell.execute_reply":"2023-09-13T08:34:43.785741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Create Pipelines #","metadata":{}},{"cell_type":"code","source":"def data_augment(image, label):\n    seed  = 42                                                       # Setting the seed ensures reproducibility; otherwise, the learning process can produce different results each time, making it hard to control.\n    image = tf.image.random_flip_left_right(image, seed=seed)        # These functions are included here to make you aware of their existence, but not all of them necessarily yield optimal performance on the given dataset.\n    image = tf.image.random_flip_up_down(image, seed=seed)\n#     image = tf.image.random_saturation(image, 0, 2, seed=seed)\n#     image = tf.image.random_brightness(image, 0.6, seed=seed)\n#     image = tf.image.random_contrast(image, 0.3, 0.5, seed=seed)\n    \n    return image, label   \n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)        # Check load_dataset function and remember that dataset = tf.data.TFRecordDataset() with its inherent parameters\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)    # Apply data_augment function\n    dataset = dataset.repeat()                                      # The repeat method is called on the dataset to make it repeat indefinitely (for all the epochs)\n    dataset = dataset.shuffle(2048)                                 # Shuffling the data is important during training to prevent the model from memorizing the order \n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)                                # Prefetch next batch while training. Thanks to this statement, data pipeline code is executed on the CPU, \n                                                                    # saving the TPU capacities for computing gradients.\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered) # 'ordered=ordered' passes the 'ordered' parameter's value from the overarching function 'get_validation_dataset'\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()                                                   # Caching the dataset means that it is temporarily stored in RAM, making it faster to access during subsequent epochs \n    dataset = dataset.prefetch(AUTO)                                            \n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):             # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items                            \n    n = [int(re.compile(r\"-([0-9]*)\\.\")      # This is a 'regular expression'. re.compile() creates here a pattern where a number appears between a hyphen and a period, \n               .search(filename)             # looks for the pattern in the filenames\n               .group(1))                    # returns what was found. re.group() regulates which part of a pattern to return: re.group(0) returns the entire matched pattern, and re.group(n) returns the respective subpattern if the pattern contains a number of them.\n                for filename in filenames]                          \n    return np.sum(n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T08:34:43.792141Z","iopub.execute_input":"2023-09-13T08:34:43.792920Z","iopub.status.idle":"2023-09-13T08:34:43.812010Z","shell.execute_reply.started":"2023-09-13T08:34:43.792885Z","shell.execute_reply":"2023-09-13T08:34:43.810780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_TRAINING_IMAGES     = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES   = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES         = count_data_items(TEST_FILENAMES)\n\nprint(f'Dataset: \\n'\n      f'{NUM_TRAINING_IMAGES} training images \\n'\n      f'{NUM_VALIDATION_IMAGES} validation images \\n'\n      f'{NUM_TEST_IMAGES} unlabeled test images')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:43.816590Z","iopub.execute_input":"2023-09-13T08:34:43.817254Z","iopub.status.idle":"2023-09-13T08:34:43.832984Z","shell.execute_reply.started":"2023-09-13T08:34:43.817179Z","shell.execute_reply":"2023-09-13T08:34:43.831977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This next cell will create the datasets (traning, validation and test)\n- These datasets are `tf.data.Dataset` objects. You can think about a dataset in TensorFlow as a *stream* of data records. \n- The training and validation sets are streams of `(image, label)` pairs.\n- The test set is a stream of `(image, idnum)` pairs; we'll use these `idnum` (ID numbers) later to make our submission `csv` file.","metadata":{}},{"cell_type":"code","source":"ds_train = get_training_dataset()\nds_valid = get_validation_dataset()\nds_test  = get_test_dataset()\n\n\nnp.set_printoptions(threshold=15, linewidth=80)          # Set the print options for NumPy to control the way arrays are displayed. This is in order to display only a part rather then all the information\n\nprint(\"Training data shapes:\")\nfor image, label in ds_train.take(3):                    # Iterate through the first 3 elements of the training dataset\n    print(image.numpy().shape, label.numpy().shape)      # .numpy() converts a TensorFlow tensor to a NumPy array\nprint(\"Training data label examples:\", label.numpy())\n\nprint ('---')\n\nprint(\"Test data shapes:\")\nfor image, idnum in ds_test.take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:44.016860Z","iopub.execute_input":"2023-09-13T08:34:44.017981Z","iopub.status.idle":"2023-09-13T08:34:53.941438Z","shell.execute_reply.started":"2023-09-13T08:34:44.017943Z","shell.execute_reply":"2023-09-13T08:34:53.940324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Explore Data #\nLet's take a moment to look at some of the images in the dataset.\n<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n    The following is a <strong> Matplotlib</strong> cell. If you are here for <strong> TensorFlow </strong> only, please go straight to <strong> Step 6</strong>. <br> </blockquote>","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data                                          # unpack the tuples of (image, label) and (image, idnum). See above read_labeled_tfrecord and read_unlabeled_tfrecord functions\n    numpy_images   = images.numpy()                                # .numpy() converts a TensorFlow tensor to a NumPy array\n    numpy_labels   = labels.numpy()\n    if numpy_labels.dtype == object:                               # Remember,that in our case`label` is tf.int64 (numeric format) and `idnum` is tf.string (bytestring, an `object`) \n        numpy_labels = [None for _ in enumerate(numpy_images)]     # So, if numpy_labels ends up carring 'idnum' values (not the 'labels'), this statement sets them to None (for test data)\n    return numpy_images, numpy_labels\n\n\n# A function to generate a title based on the predicted and true target values\ndef title_from_label_and_target(label, correct_label):             # it takes predictions (labels) and true values (correct_label) as arguments\n    if correct_label is None:                                      # if we deal with the test set, where no correct_labels are availible \n        return CLASSES[label], True                                # it simply returns the prediction\n    \n    correct = (label == correct_label)                             # if target value (correct_label) is availible, it compares it with the prediction and returns a boolean value (True/False)\n    \n    return \"{} [{}{}{}]\".format(CLASSES[label],                    # returns the prediction\n            'OK' if correct else 'NO',                             # 'OK' if it is True, 'NO'          if it is False\n            u\"\\u2192\" if not correct else '',                      # ''   if it is True, 'â†’'           if it is False \n            CLASSES[correct_label] if not correct else ''),correct # ''   if it is True, correct_label if it is False, separate value for 'True' or 'False'\n                                                         \n                                                            \n\n    \n# a function to display a single flower image with a title\ndef display_one_flower(image, title, subplot,                       # subplot is what you need to display several pictures at once (on one plot)\n                       red=False, titlesize=16):\n    plt.subplot(*subplot)                                           # '*subplot' syntax unpacks the values in the subplot tuple (rows, columns, index) that specify the subplot layout \n    plt.axis('off')\n    plt.imshow(image)                                               # plt.imshow stands for 'show image'\n    if len(title) > 0:                                                    # if title is avaliable\n        plt.title(title,                                                  # set parameters for this title's display\n          fontsize = int(titlesize) if not red else int(titlesize/1.2),   # bigger fontsize for correct (black) titles, smaller fontsize for the wrong (red) titles\n          color='red' if red else 'black',                                # depending on the argument passed to the function\n          fontdict={'verticalalignment':'center'}, \n          pad=int(titlesize/1.5))\n    \n    return (subplot[0],                                                   # the number of rows in the subplot grid\n            subplot[1],                                                   # the number of columns in the subplot grid\n            subplot[2]+1)                                                 # the current index (position) within the grid. +1 makes it an iterator: each time you call this funtion, it moves to the next image\n    \n\n# this function makes several pictures appear on the screen at the same time\ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This functions works with following settings:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    images, labels = batch_to_numpy_images_and_labels(databatch) # data\n    if labels is None:\n        labels = [None for _ in enumerate(images)]               # creates a list of None values with the same length as the images list: to ensure that there is a label for each image.\n    rows = int(math.sqrt(len(images)))                           # auto-squaring: this will drop data from the display that does not fit into square or square-ish rectangle\n    cols = len(images)//rows                                     # calculates the number of columns based on the number of rows and the total number of images. It uses integer division (//) to ensure that the grid is as square as possible.\n        \n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot = (rows,cols,1)                                      # you allready know that subplot has three parameters: (rows, columns, index)\n    if rows < cols:                                              # if there are more columns then rows\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))          # set portrait (tall) orientation\n    else:                                                        # if the are more rows then colums\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))          # set landscape (wide) orientation\n    \n    # display\n    display_dict = zip(images[:rows*cols], labels[:rows*cols])                      # a dictionary with a subset of images as keys and a subset of labels as values. The subsets start from the beginning and contain rows*cols elements \n    for i, (image, label) in enumerate (display_dict):                              # an iterator\n        title = '' if label is None else CLASSES[label]                             # determine the title for the subplot based on the label\n        correct = True                                                              # set the default value for 'correct'\n        if predictions is not None:                                                 # if predictions are passed to the function\n            title, correct = title_from_label_and_target(predictions[i], label)     # apply the above formular, passing the predictions' indexes from the iterator and the corresponding labeles \n        dynamic_titlesize  = FIGSIZE*SPACING/max(rows,cols)*40+3                    # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, \n                                     not correct,                                   # this is a value for parameter 'red' of display_one_flower function. So, if the prediction is False (correct=False), were turn it around (not correct) and pass True (red=True) to the function\n                                     titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()                                                # ensure that the subplots (in this case, the displayed images and titles) fit within the figure without overlapping or being cut off.\n    if label is None and predictions is None:                         # if there are no predictions and true labels     \n        plt.subplots_adjust(wspace=0, hspace=0)                       # no spacing between the images             \n    else:                                                             # otherwise\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)           # make spaces\n    plt.show()\n\n\ndef display_training_curves(training, validation, title, subplot):    # takes training data, validation data, title for the subplot, subplot number\n    if subplot%10==1:                                                 # if subplot is a multiple of 10 plus 1 (e.g., 1, 11, 21, etc.) \n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')            # creates a new figure with a specified figsize and background color (light gray)\n        plt.tight_layout()                                            # ensure that the subplots fit within the figure without overlapping or being cut off.\n    ax = plt.subplot(subplot)                                         # creates a subplot within the current figure with the specified subplot number.\n    ax.set_facecolor('#F8F8F8')                                       # sets the background color of the subplot (very light gray color)\n    ax.plot(training)                                                 # display curve for training set\n    ax.plot(validation)                                               # display curve for validation set\n    ax.set_title('model '+ title)                                     # the titles will be 'loss' and 'accuracy'\n    ax.set_ylabel(title)\n    ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T09:11:53.223319Z","iopub.execute_input":"2023-09-13T09:11:53.223818Z","iopub.status.idle":"2023-09-13T09:11:53.249751Z","shell.execute_reply.started":"2023-09-13T09:11:53.223785Z","shell.execute_reply":"2023-09-13T09:11:53.248770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- You can display a single batch of images from a dataset with another of our helper functions. \n- The next cell will turn the dataset into an iterator of batches of 20 images.\n- Use the Python `next` function to pop out the next batch in the stream and display it with the helper function.\n- By defining `ds_iter` and `one_batch` in separate cells, you only need to rerun the second cell to see a new batch of images.","metadata":{}},{"cell_type":"code","source":"ds_iter = iter(ds_train.unbatch().batch(20))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:54.004279Z","iopub.execute_input":"2023-09-13T08:34:54.004560Z","iopub.status.idle":"2023-09-13T08:34:54.043289Z","shell.execute_reply.started":"2023-09-13T08:34:54.004535Z","shell.execute_reply":"2023-09-13T08:34:54.042406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_batch = next(ds_iter)\ndisplay_batch_of_images(one_batch)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:34:54.045083Z","iopub.execute_input":"2023-09-13T08:34:54.045343Z","iopub.status.idle":"2023-09-13T08:34:59.344287Z","shell.execute_reply.started":"2023-09-13T08:34:54.045320Z","shell.execute_reply":"2023-09-13T08:34:59.342803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Define Model #\n\nNow we're ready to create a neural network for classifying images! \n- We'll use what's known as **transfer learning**: take a pretrained heavy model (base) and set your keras model on top of it (head)\n- We will use **Xception** as the base (cause it performs well on this dataset). Run the cell below to see the list of avalible bases in Keras\n- The distribution strategy we created earlier contains a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with), `strategy.scope`. When using a TPU, it's important to define your model in a strategy.scope() context.","metadata":{}},{"cell_type":"code","source":"# the list of avalible pretrained models (bases) in Keras\n', '.join(tf.keras.applications.__dir__())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:35:11.472384Z","iopub.execute_input":"2023-09-13T08:35:11.472785Z","iopub.status.idle":"2023-09-13T08:35:11.480374Z","shell.execute_reply.started":"2023-09-13T08:35:11.472753Z","shell.execute_reply":"2023-09-13T08:35:11.479353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers.experimental import preprocessing\n\nwith strategy.scope():\n    pretrained_model = tf.keras.applications.Xception(\n                       weights='imagenet',                   \n                       include_top=False,                         # we will build our own head on top of this base, so we tell the the strategy to 'decapitate' the base\n                       input_shape=[*IMAGE_SIZE, 3]               # '*' unpacks the IMAGE_SIZE tuple, passing it's two elements as separate values\n    )\n    pretrained_model.trainable = False                            # transfer learning\n    \n    model = tf.keras.Sequential([                                 # Here is our eventual model:\n#         preprocessing.RandomFlip('horizontal'),                   # add another data-augmentation layer \n#         preprocessing.RandomFlip(mode='vertical'),                # add another data-augmentation layer \n#         preprocessing.RandomWidth(factor=0.15),                   # add another data-augmentation layer \n#         preprocessing.RandomRotation(factor=0.20),                # add another data-augmentation layer\n        pretrained_model,                                         # add the pretrained base  \n        tf.keras.layers.GlobalAveragePooling2D(),                 # attach a new head (GlobalAveragePooling averages feature maps produced by the base down to a single value per feature. Which is just right for a classification)\n        tf.keras.layers.Dense(                                    # to act as a classifier\n                            len(CLASSES),                         # number of neurons in the output layer corresponds to the number of classes\n                            activation='softmax')                 # this is the activation function you want to use for a multi-class classification task\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:35:27.118086Z","iopub.execute_input":"2023-09-13T08:35:27.118442Z","iopub.status.idle":"2023-09-13T08:35:29.792648Z","shell.execute_reply.started":"2023-09-13T08:35:27.118413Z","shell.execute_reply":"2023-09-13T08:35:29.791665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer= 'nadam',                          # Nesterov-accelerated Adaptive Moment Estimation (nadam) is an extension of Adaptive Moment Estimation (adam)\n    loss =   'sparse_categorical_crossentropy',  # The one you need for a multi-class classification\n    metrics=['sparse_categorical_accuracy'],     # The one you need for a multi-class classification\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:35:29.794981Z","iopub.execute_input":"2023-09-13T08:35:29.795592Z","iopub.status.idle":"2023-09-13T08:35:29.817415Z","shell.execute_reply.started":"2023-09-13T08:35:29.795556Z","shell.execute_reply":"2023-09-13T08:35:29.816212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Training\nWe will make some **callbacks** first\n- A callback is a .fit() parameter, where you can pass different objects:\n- **`Learning Rate Schedule`**: adjusts the learning rate e.g. after a certain number of epochs or when the training loss plateaus or else\n- **`Early Stopping`**: stops learning if there is no improvement after several epochs\n- There are [many other options](https://keras.io/api/callbacks/). But we will make these two here.","metadata":{}},{"cell_type":"code","source":"# callback 1: a customary learning rate schedule\n\nEPOCHS = 30 \n\ndef exponential_lr(epoch,                                   # The current training epoch\n                   start_lr = 0.00001,                      # The initial learning rate\n                   min_lr = 0.00001,                        # The minimum learning rate\n                   max_lr = 0.00005,                        # The maximum learning rate\n                   rampup_epochs = 5,                       # The number of epochs for a linear increase in learning rate\n                   sustain_epochs = 0,                      # The number of epochs to sustain the maximum learning rate\n                   exp_decay = 0.8):                        # The exponential decay factor for learning rate reduction\n\n    # calculates the learning rate for a given epoch based on the provided parameters\n    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay): \n        \n        if epoch < rampup_epochs:                             # For epochs less than rampup_epochs, the learning rate increases from start_lr to max_lr.\n            lr = ((max_lr - start_lr) /\n                  rampup_epochs * epoch + start_lr)\n        \n        elif epoch < rampup_epochs + sustain_epochs:          # From 'rampup_epochs' till 'rampup_epochs + sustain_epochs', the learning rate remains constant at max_lr\n            lr = max_lr\n        \n        else:                                                 # exponential decay towards min_lr\n            lr = ((max_lr - min_lr) *\n                  exp_decay**(epoch - rampup_epochs - sustain_epochs) +\n                  min_lr)\n        return lr\n    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n\n# This is what it was all about. We pass our customary funtion to the keras LearningRateScheduler to create a callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(exponential_lr, verbose=True)  \n\n# plot our customary learning rate per epoch\nrng = [i for i in range(EPOCHS)]     \ny = [exponential_lr(x) for x in rng]\nplt.plot(rng, y)\nprint(f'Learning rate schedule: \\n'\n      f'from {y[0]:.3g} \\n'\n      f'to {max(y):.3g} \\n'\n      f'and then back to {y[-1]:.3g}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T08:35:29.820218Z","iopub.execute_input":"2023-09-13T08:35:29.820525Z","iopub.status.idle":"2023-09-13T08:35:30.124031Z","shell.execute_reply.started":"2023-09-13T08:35:29.820483Z","shell.execute_reply":"2023-09-13T08:35:30.122007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callback 2: it will stop the training when there is no improvement in the validation loss for three consecutive epochs. \nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:35:30.125589Z","iopub.execute_input":"2023-09-13T08:35:30.125954Z","iopub.status.idle":"2023-09-13T08:35:30.132016Z","shell.execute_reply.started":"2023-09-13T08:35:30.125920Z","shell.execute_reply":"2023-09-13T08:35:30.130691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 30                                           # Early stopping should break it sooner\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE   # Batches per epoch\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    callbacks=[lr_callback, early_stopping]           # Here is where our callbacks go\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T08:35:30.282055Z","iopub.execute_input":"2023-09-13T08:35:30.282342Z","iopub.status.idle":"2023-09-13T09:02:25.848998Z","shell.execute_reply.started":"2023-09-13T08:35:30.282317Z","shell.execute_reply":"2023-09-13T09:02:25.847959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7: Evaluate Predictions\n1. We will draw a **plot of the learning progress** to see where and if it converges\n2. We will apply our **display_batch_of_images()** function to see the flowers, their predicted and true classes. **Visual validation** can help reveal patterns of images the model has trouble with.","metadata":{}},{"cell_type":"code","source":"# Plot the learning progress\ndisplay_training_curves(\n    history.history['loss'],\n    history.history['val_loss'],\n    'loss',\n    211,\n)\n\ndisplay_training_curves(\n    history.history['sparse_categorical_accuracy'],\n    history.history['val_sparse_categorical_accuracy'],\n    'accuracy',\n    212,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T09:03:29.543469Z","iopub.execute_input":"2023-09-13T09:03:29.544423Z","iopub.status.idle":"2023-09-13T09:03:30.281914Z","shell.execute_reply.started":"2023-09-13T09:03:29.544388Z","shell.execute_reply":"2023-09-13T09:03:30.280608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visual validation\ndataset = get_validation_dataset()\ndataset = dataset.unbatch().batch(20)     # Display 20 images at a time. Fill free to put your number\nbatch = iter(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T09:13:31.978922Z","iopub.execute_input":"2023-09-13T09:13:31.979494Z","iopub.status.idle":"2023-09-13T09:13:42.275686Z","shell.execute_reply.started":"2023-09-13T09:13:31.979452Z","shell.execute_reply":"2023-09-13T09:13:42.274620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run the cell again to see another set\nimages, labels = next(batch)\nprobabilities = model.predict(images)\npredictions = np.argmax(probabilities, axis=-1)\ndisplay_batch_of_images((images, labels), predictions)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T09:13:42.277969Z","iopub.execute_input":"2023-09-13T09:13:42.278485Z","iopub.status.idle":"2023-09-13T09:13:45.582882Z","shell.execute_reply.started":"2023-09-13T09:13:42.278411Z","shell.execute_reply":"2023-09-13T09:13:45.581950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 8: Make Test Predictions","metadata":{}},{"cell_type":"code","source":"test_ds = get_test_dataset(ordered=True)\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)   # Create a dataset containing only test images\nprobabilities = model.predict(test_images_ds)              # Predict class probabilities for the test images\npredictions = np.argmax(probabilities, axis=-1)            # Find the class with the highest probability for each image: \nprint(predictions)                                         # 'predictions' is a sequence of matrixes, where each matrix (idividual prediction) consists of 2 vectors: (1) all the classes (indexes) and (2) probabilities of an image to be that class\n                                                           # .argmax() returns index of a maximum value, i.e. class related to the highest probability value","metadata":{"execution":{"iopub.status.busy":"2023-09-13T09:13:59.965771Z","iopub.execute_input":"2023-09-13T09:13:59.966138Z","iopub.status.idle":"2023-09-13T09:14:41.013557Z","shell.execute_reply.started":"2023-09-13T09:13:59.966109Z","shell.execute_reply":"2023-09-13T09:14:41.012393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll generate a file `submission.csv`. This file is what you'll submit to get your score on the leaderboard.","metadata":{}},{"cell_type":"code","source":"print('Generating submission.csv file...')\n\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()                # Get image ids from test set\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')  # convert the ids to unicode \n\n# Write the submission file\nnp.savetxt(\n    'submission.csv',\n    np.rec.fromarrays([test_ids, predictions]),\n    fmt=['%s', '%d'],\n    delimiter=',',\n    header='id,label',\n    comments='',\n)\n\n# Look at the first few predictions\n!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2023-09-13T09:14:54.153233Z","iopub.execute_input":"2023-09-13T09:14:54.153618Z","iopub.status.idle":"2023-09-13T09:15:00.842071Z","shell.execute_reply.started":"2023-09-13T09:14:54.153589Z","shell.execute_reply":"2023-09-13T09:15:00.840869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 9: Make a submission #\n\nIf you haven't already, create your own editable copy of this notebook by clicking on the **Copy and Edit** button in the top right corner. Then, submit to the competition by following these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n","metadata":{}},{"cell_type":"markdown","source":"# Step 10. Upvote the notebook\n\nI hope you enjoyed the journey, and it was indeed a pleasure as promissed. Please, upvote the notebook in case you read this far and feel like compliting a real-life TensorFlow project. Comments are also mostly welcome. ","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161321) to chat with other Learners.*","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}