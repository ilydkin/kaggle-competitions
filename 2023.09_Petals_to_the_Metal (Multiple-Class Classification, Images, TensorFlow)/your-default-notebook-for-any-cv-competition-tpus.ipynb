{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Your Default Notebook for any Computer Vision Competition, with TPUs","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://user-images.githubusercontent.com/115424463/267539825-8e845684-c60a-4bc9-838d-f0b800e317b9.jpeg\" alt=\"'Petals to the Metal' by AI\">\n\n### Picture: Petals to the Metal","metadata":{}},{"cell_type":"markdown","source":"This notebook serves several purposes:\n\n1. It aims to ease the transition from the [**Computer Vision**](https://www.kaggle.com/learn/computer-vision) course toimage classification competitions, using [**Petals to the Metal**](https://www.kaggle.com/c/tpu-getting-started) as an example. To achieve this, the code in this notebook is extensively commented to make it accessible to individuals regardless of their prior experience. We have maintained a high-level approach wherever possible.\n\n2. Its objective is to serve as a collection of logical blocks and utility scripts that you can use for exploratory data analysis (EDA) and model training, allowing you to design and implement your own strategies.\n\n3. It includes an example of a custom convolutional neural network (CNN) to illustrate the theoretical concepts from the Computer Vision course.\n\n4. It introduces several essential tools that are typically used in competitions: \n   - *Callbacks (EarlyStopping, LearningRateSchedule, Checkpoints)*\n   - *Transfer learning*\n   - *Building an ensemble of multiple models* <br>\n  \nCredits:\n- We extend our immense gratitude to the authors of the **Computer Vision** course and [**Create Your First Sumbission**](https://www.kaggle.com/code/ryanholbrook/create-your-first-submission) notebook. It's truly miraculous that such materials are available for free!\n- Several insights and functions were taken from [**George Zoto's** notebook](https://www.kaggle.com/code/georgezoto/computer-vision-petals-to-the-metal), which we strongly recommend for a thorough perusal as both comprehensive and inspiring. \n\n<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n <strong> Fork this notebook </strong> by clicking on the Copy and Edit button in the top right corner. It is designed to improve your visual comprehension, which is most apparent when you are in edit mode.<br>\n    <strong> Please upvote </strong> and comment to keep me motivated and feel like being a part of big Data Science World. <br> </blockquote>\n    <br> </blockquote>   ","metadata":{}},{"cell_type":"markdown","source":"# Step 0: Imports","metadata":{}},{"cell_type":"code","source":"# !pip install --upgrade pip\n# !pip install -U tensorflow == 2.11.0  \n\nimport math, re, os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\nfrom sklearn.metrics import f1_score\n\nprint(f'TensorFlow version: {tf.__version__}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:52:44.549020Z","iopub.execute_input":"2023-09-18T17:52:44.549387Z","iopub.status.idle":"2023-09-18T17:52:52.216645Z","shell.execute_reply.started":"2023-09-18T17:52:44.549357Z","shell.execute_reply":"2023-09-18T17:52:52.215715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Connect to Tensor Processing Units (TPUs)\n\nKaggle provides a limited access to 3 types of processing units, avaliale for your models' training.\n- Central Processing Units (**CPUs**)\n- Graphics Processing Units (**GPUs**)\n- Tensor Processing Units (**TPUs**)\n\nHere is an [**article**](https://towardsdatascience.com/when-to-use-cpus-vs-gpus-vs-tpus-in-a-kaggle-competition-9af708a8c3eb) to help you figure out which is which. Long story short, \"GPUs are a great alternative to CPUs when you want to speed up a variety of data science workflows, and TPUs are best when you specifically want to train a machine learning model as fast as you possibly can\". But you will have to work on the code to make your data digestable for a TPU.\n\nA TPU has **eight cores** (it's like having eight GPUs in one machine). With **distribution strategy**, we instruct TensorFlow on how to utilize all these cores simultaneously. We will employ this object when constructing our neural network model: it will distribute the training by generating eight distinct *replicas* of the model, one for each core.","metadata":{}},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # An attempt to detect an avaliable TPU (to 'resolve a TPU cluster')\n    print('Running on TPU ', tpu.master())                     # Tell the world that the attempt was a success!\nexcept ValueError:\n    tpu = None\n\nif tpu:                                                        # If TPU was detected\n    tf.config.experimental_connect_to_cluster(tpu)             # connect to the TPU cluster \n    tf.tpu.experimental.initialize_tpu_system(tpu)             # run the cluster\n    strategy = tf.distribute.TPUStrategy(tpu)                  # create a distribution strategy for TPU training\nelse:\n    strategy = tf.distribute.get_strategy()                    # If no TPU was found, use the default distribution strategy for CPU or GPU\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)             # Tell the world which strategy it is","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:52:52.218584Z","iopub.execute_input":"2023-09-18T17:52:52.219372Z","iopub.status.idle":"2023-09-18T17:52:52.232671Z","shell.execute_reply.started":"2023-09-18T17:52:52.219335Z","shell.execute_reply":"2023-09-18T17:52:52.231498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set how many files can be processed simultaniously. This will be 16 with TPU off and 128 (=16*8) with TPU on\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:52:52.234471Z","iopub.execute_input":"2023-09-18T17:52:52.234909Z","iopub.status.idle":"2023-09-18T17:52:52.240152Z","shell.execute_reply.started":"2023-09-18T17:52:52.234821Z","shell.execute_reply":"2023-09-18T17:52:52.239022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Retrieve, Load and Format Data\n\n- When used with TPUs, datasets need to be stored in a [Google Cloud Storage bucket](https://cloud.google.com/storage/) (**GCS**). You can use data from any public GCS bucket by giving its path (like `'/kaggle/input'`). \n- You can use data from any public dataset here on Kaggle in just the same way. If you'd like to use data from one of your private datasets, see [here](https://www.kaggle.com/docs/tpu#tpu3pt5).\n- When used with TPUs, datasets are serialized into [TFRecords](https://www.kaggle.com/ryanholbrook/tfrecords-basics). This is a format for distributing data to each of the TPUs cores.","metadata":{}},{"cell_type":"code","source":"# Here we create lists of paths to our training, validation and test files\nfrom kaggle_datasets import KaggleDatasets\n\nGCS_DS_PATH     = KaggleDatasets().get_gcs_path('tpu-getting-started')   # You can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\nGCS_DS_PATH_EXT = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec') # More data from a side source! If you get an error here, add 'tf-flower-photo-tfrec' in the 'Add data' tab\n\nIMAGE_SIZE = [192, 192]                                                  # This is the size for GPU. For TPU use [512, 512]\n                 \nGCS_PATH_SELECT = {                                                      # Images of different sizes are strored in different directories. The dictionary connects the sizes to the paths\n    192: '/tfrecords-jpeg-192x192',\n    224: '/tfrecords-jpeg-224x224',\n    331: '/tfrecords-jpeg-331x331',\n    512: '/tfrecords-jpeg-512x512'\n}\n\nGCS_PATH_PER_SIZE = GCS_PATH_SELECT[IMAGE_SIZE[0]]                       # Define the path to the directory depending on the IMAGE_SIZE\nGCS_PATH_ORIGINAL = GCS_DS_PATH + GCS_PATH_PER_SIZE                      # This is where the original data for the competition dwells\n\nIMAGENET_FILES    = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/imagenet'    + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\nINATURELIST_FILES = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/inaturalist' + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\nOPENIMAGE_FILES   = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/openimage'   + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\nOXFORD_FILES      = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/oxford_102'  + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\nTENSORFLOW_FILES  = tf.io.gfile.glob(GCS_DS_PATH_EXT + '/tf_flowers'  + GCS_PATH_PER_SIZE + '/*.tfrec') # More data from a side source!\n\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH_ORIGINAL  + '/train/*.tfrec')  # Get the list of file paths for training TFRecords\nTRAINING_FILENAMES = TRAINING_FILENAMES + IMAGENET_FILES + INATURELIST_FILES + OPENIMAGE_FILES + OXFORD_FILES + TENSORFLOW_FILES  # Add the extra data\n\n\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH_ORIGINAL + '/val/*.tfrec')   # Get the list of file paths for validation TFRecords\nTEST_FILENAMES       = tf.io.gfile.glob(GCS_PATH_ORIGINAL + '/test/*.tfrec')  # Get the list of file paths for testing TFRecords","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:52:52.243915Z","iopub.execute_input":"2023-09-18T17:52:52.244246Z","iopub.status.idle":"2023-09-18T17:52:58.111226Z","shell.execute_reply.started":"2023-09-18T17:52:52.244222Z","shell.execute_reply":"2023-09-18T17:52:58.110196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These are our classification labels \nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']      ","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:52:58.112964Z","iopub.execute_input":"2023-09-18T17:52:58.113401Z","iopub.status.idle":"2023-09-18T17:52:58.122422Z","shell.execute_reply.started":"2023-09-18T17:52:58.113366Z","shell.execute_reply":"2023-09-18T17:52:58.121506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code allows to create sets of TFRecords. This is a special data format suitable for processing on TPU.\nIf not for TPU, we could have easily use `keras.preprocessing.image_dataset_from_directory()`","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE                     # Configure Auto-tuning for better performance. To be applied in many functions below                                                                                                                                # 100 - 102\n\ndef decode_image(image_data):                            \n    image = tf.image.decode_jpeg(image_data, channels=3) # Decode the JPEG image to a tensor with 3 color channels (red, green, blue)\n    image = tf.cast(image, tf.float32) / 255.0           # Convert pixel values to floating-point numbers in the range [0, 1]\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])          # Reshape the image tensor to match the specified IMAGE_SIZE\n                                                         # This step ensures that all images have the same dimensions for consistency\n    return image\n\n# This function reads a labeled TFRecord file and returns the image and its corresponding label (to be applied on training and validation sets)\ndef read_labeled_tfrecord(example):                                     # example: A single labeled TFRecord file (labled picture to be used for training and validation)\n    LABELED_TFREC_FORMAT = {                                            # setting a dictionary that defines the format of a TFRecord (names and dtypes of its features)\n        \"image\": tf.io.FixedLenFeature([], tf.string),                  # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),                   # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) # Parse the single TFRecord example according to the specified format.\n    image = decode_image(example['image'])                              # Decode the 'image' feature of a TFRecord file using the 'decode_image' function (previously defined)                         \n    label = tf.cast(example['class'], tf.int32)                         # tf.cast converts tensors from one data type to another. Here it ensures that all elements are integers\n    return image, label                                                 # returns a dataset of (image, label) pairs. In Python you get a tuple with this syntaxis automatically\n\n\n# This function reads an unlabeled TFRecord file and returns the image and its ID (to be applied on the test set)\ndef read_unlabeled_tfrecord(example):                       \n    UNLABELED_TFREC_FORMAT = {                             \n        \"image\": tf.io.FixedLenFeature([], tf.string),     \n        \"id\": tf.io.FixedLenFeature([], tf.string),                       # Class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT) # Parse the single TFRecord example according to the specified format.\n    image = decode_image(example['image'])                                # Decode the 'image' feature of a TFRecord example using the 'decode_image' function (previously defined)\n    idnum = example['id']\n    return image, idnum                                                   # Returns a dataset of (image, id) pairs. In Python you get a tuple with this syntaxis automatically\n\n\n# Read from TFRecords. For optimal performance, reading from multiple files at once and disregarding data order.\ndef load_dataset(filenames, labeled=True, ordered=False):                 # We set values for 'labeled' and 'ordered' in the definition of the function to use them by default. However, we reserve an option to pass different values to these parameters.\n    \n    options = tf.data.Options()                                           # Creating an objects here looks like a TensorFlow reference code. It is literally the same as in the documentation\n    if not ordered:                                                       # If the 'ordered' parameter is 'False' (the default) and hasn't been explicitly set to 'True' when passed to the function\n        options.deterministic = False                                     # Disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files. This 'num_parallel_reads=AUTO' parameter of tf.data.TFRecordDataset() will be used many times in the below code.\n    dataset = dataset.with_options(options)                               # uses data as soon as it streams in, rather than in its original order\n                          \n                          # returns a dataset of (image, label) pairs if labeled=True\n    dataset = dataset.map(read_labeled_tfrecord if labeled \\\n                          # returns a dataset of (image, id) pairs if labeled=False\n                          else read_unlabeled_tfrecord,\n                          num_parallel_calls=AUTO)                        \n    return dataset","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-18T17:52:58.123991Z","iopub.execute_input":"2023-09-18T17:52:58.124668Z","iopub.status.idle":"2023-09-18T17:52:58.141086Z","shell.execute_reply.started":"2023-09-18T17:52:58.124631Z","shell.execute_reply":"2023-09-18T17:52:58.140016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Create Pipelines #","metadata":{}},{"cell_type":"code","source":"def data_augment(image, label):\n    seed  = 42                                                       # Setting the seed ensures reproducibility; otherwise, the learning process can produce different results each time, making it hard to control.\n    image = tf.image.random_flip_left_right(image, seed=seed)        # These functions are included here to make you aware of their existence, but not all of them necessarily yield optimal performance on the given dataset.\n    image = tf.image.random_flip_up_down(image, seed=seed)\n#   image = tf.image.random_saturation(image, 0, 2, seed=seed)       # It doesn't seem a great idea to change colours of flowers. But it could work on images of a different kind\n#   image = tf.image.random_brightness(image, 0.6, seed=seed)\n#   image = tf.image.random_contrast(image, 0.3, 0.5, seed=seed)\n    \n    return image, label   \n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)        # Check load_dataset function and recall that 'dataset = tf.data.TFRecordDataset()'' with its inherent parameters\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)    # Apply data_augment function\n    dataset = dataset.repeat()                                      # The repeat method is called on the dataset to make it repeat indefinitely (for all the epochs)\n    dataset = dataset.shuffle(2048)                                 # Shuffling the data is important during training to prevent the model from memorizing the order \n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)                                # Prefetch next batch while training. Thanks to this statement, data pipeline code is executed on the CPU, \n                                                                    # saving the TPU capacities for computing gradients.\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered) # 'ordered=ordered' passes the 'ordered' parameter's value from the overarching function 'get_validation_dataset'\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()                                                   # Caching the dataset means that it is temporarily stored in RAM, making it faster to access during subsequent epochs \n    dataset = dataset.prefetch(AUTO)                                            \n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):             # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items                            \n    n = [int(re.compile(r\"-([0-9]*)\\.\")      # This is a 'regular expression'. re.compile() creates here a pattern where a number appears between a hyphen and a period, \n               .search(filename)             # looks for the pattern in the filenames\n               .group(1))                    # returns what was found. re.group() regulates which part of a pattern to return: re.group(0) returns the entire matched pattern, and re.group(n) returns the respective subpattern if the pattern contains a number of them.\n                for filename in filenames]                          \n    return np.sum(n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-18T17:52:58.142532Z","iopub.execute_input":"2023-09-18T17:52:58.143029Z","iopub.status.idle":"2023-09-18T17:52:58.157221Z","shell.execute_reply.started":"2023-09-18T17:52:58.142996Z","shell.execute_reply":"2023-09-18T17:52:58.156259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_TRAINING_IMAGES     = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES   = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES         = count_data_items(TEST_FILENAMES)\n\nprint(f'Dataset: \\n'\n      f'{NUM_TRAINING_IMAGES} training images \\n'\n      f'{NUM_VALIDATION_IMAGES} validation images \\n'\n      f'{NUM_TEST_IMAGES} unlabeled test images')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:52:58.159610Z","iopub.execute_input":"2023-09-18T17:52:58.159867Z","iopub.status.idle":"2023-09-18T17:52:58.174210Z","shell.execute_reply.started":"2023-09-18T17:52:58.159845Z","shell.execute_reply":"2023-09-18T17:52:58.172797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This next cell will create the datasets (traning, validation and test)\n- These datasets are `tf.data.Dataset` objects. You can think about a dataset in TensorFlow as a *stream* of data records. \n- The training and validation sets are streams of `(image, label)` pairs.\n- The test set is a stream of `(image, idnum)` pairs; we'll use these `idnum` (ID numbers) later to make our submission `csv` file.","metadata":{}},{"cell_type":"code","source":"ds_train = get_training_dataset()\nds_valid = get_validation_dataset()\nds_test  = get_test_dataset()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:52:58.177801Z","iopub.execute_input":"2023-09-18T17:52:58.178058Z","iopub.status.idle":"2023-09-18T17:53:01.529006Z","shell.execute_reply.started":"2023-09-18T17:52:58.178035Z","shell.execute_reply":"2023-09-18T17:53:01.527988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at the data shapes\nnp.set_printoptions(threshold=15, linewidth=80)          # Set the print options for NumPy to control the way arrays are displayed. This is in order to display only a part rather then all the information\n\nprint(\"Training data shapes:\")\nfor image, label in ds_train.take(3):                    # Iterate through the first 3 elements of the training dataset\n    print(image.numpy().shape, label.numpy().shape)      # .numpy() converts a TensorFlow tensor to a NumPy array\nprint(\"Training data label examples:\", label.numpy())\n\nprint ('---')\n\nprint(\"Test data shapes:\")\nfor image, idnum in ds_test.take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:53:01.532837Z","iopub.execute_input":"2023-09-18T17:53:01.533123Z","iopub.status.idle":"2023-09-18T17:53:09.490244Z","shell.execute_reply.started":"2023-09-18T17:53:01.533098Z","shell.execute_reply":"2023-09-18T17:53:09.489369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Explore Data #\nLet's take a moment to look at some of the images in the dataset.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data                                          # unpack the tuples of (image, label) and (image, idnum). See above read_labeled_tfrecord and read_unlabeled_tfrecord functions\n    numpy_images   = images.numpy()                                # .numpy() converts a TensorFlow tensor to a NumPy array\n    numpy_labels   = labels.numpy()\n    if numpy_labels.dtype == object:                               # Remember,that in our case`label` is tf.int64 (numeric format) and `idnum` is tf.string (bytestring, an `object`) \n        numpy_labels = [None for _ in enumerate(numpy_images)]     # So, if numpy_labels ends up carring 'idnum' values (not the 'labels'), this statement sets them to None (for test data)\n    return numpy_images, numpy_labels\n\n\n# A function to generate a title based on the predicted and true target values\ndef title_from_label_and_target(label, correct_label):             # it takes predictions (labels) and true values (correct_label) as arguments\n    if correct_label is None:                                      # if we deal with the test set, where no correct_labels are availible \n        return CLASSES[label], True                                # it simply returns the prediction\n    \n    correct = (label == correct_label)                             # if target value (correct_label) is availible, it compares it with the prediction and returns a boolean value (True/False)\n    \n    return \"{} [{}{}{}]\".format(CLASSES[label],                    # returns the prediction\n            'OK' if correct else 'NO',                             # 'OK' if it is True, 'NO'          if it is False\n            u\"\\u2192\" if not correct else '',                      # ''   if it is True, '→'           if it is False \n            CLASSES[correct_label] if not correct else ''),correct # ''   if it is True, correct_label if it is False, separate value for 'True' or 'False'\n                                                         \n                                                            \n\n    \n# a function to display a single flower image with a title\ndef display_one_flower(image, title, subplot,                       # subplot is what you need to display several pictures at once (on one plot)\n                       red=False, titlesize=16):\n    plt.subplot(*subplot)                                           # '*subplot' syntax unpacks the values in the subplot tuple (rows, columns, index) that specify the subplot layout \n    plt.axis('off')\n    plt.imshow(image)                                               # plt.imshow stands for 'show image'\n    if len(title) > 0:                                                    # if title is avaliable\n        plt.title(title,                                                  # set parameters for this title's display\n          fontsize = int(titlesize) if not red else int(titlesize/1.2),   # bigger fontsize for correct (black) titles, smaller fontsize for the wrong (red) titles\n          color='red' if red else 'black',                                # depending on the argument passed to the function\n          fontdict={'verticalalignment':'center'}, \n          pad=int(titlesize/1.5))\n    \n    return (subplot[0],                                                   # the number of rows in the subplot grid\n            subplot[1],                                                   # the number of columns in the subplot grid\n            subplot[2]+1)                                                 # the current index (position) within the grid. +1 makes it an iterator: each time you call this funtion, it moves to the next image\n    \n\n# this function makes several pictures appear on the screen at the same time\ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This functions works with following settings:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    images, labels = batch_to_numpy_images_and_labels(databatch) # data\n    if labels is None:\n        labels = [None for _ in enumerate(images)]               # creates a list of None values with the same length as the images list: to ensure that there is a label for each image.\n    rows = int(math.sqrt(len(images)))                           # auto-squaring: this will drop data from the display that does not fit into square or square-ish rectangle\n    cols = len(images)//rows                                     # calculates the number of columns based on the number of rows and the total number of images. It uses integer division (//) to ensure that the grid is as square as possible.\n        \n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot = (rows,cols,1)                                      # you allready know that subplot has three parameters: (rows, columns, index)\n    if rows < cols:                                              # if there are more columns then rows\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))          # set portrait (tall) orientation\n    else:                                                        # if the are more rows then colums\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))          # set landscape (wide) orientation\n    \n    # display\n    display_dict = zip(images[:rows*cols], labels[:rows*cols])                      # a dictionary with a subset of images as keys and a subset of labels as values. The subsets start from the beginning and contain rows*cols elements \n    for i, (image, label) in enumerate (display_dict):                              # an iterator\n        title = '' if label is None else CLASSES[label]                             # determine the title for the subplot based on the label\n        correct = True                                                              # set the default value for 'correct'\n        if predictions is not None:                                                 # if predictions are passed to the function\n            title, correct = title_from_label_and_target(predictions[i], label)     # apply the above formular, passing the predictions' indexes from the iterator and the corresponding labeles \n        dynamic_titlesize  = FIGSIZE*SPACING/max(rows,cols)*40+3                    # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, \n                                     not correct,                                   # this is a value for parameter 'red' of display_one_flower function. So, if the prediction is False (correct=False), were turn it around (not correct) and pass True (red=True) to the function\n                                     titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()                                                # ensure that the subplots (in this case, the displayed images and titles) fit within the figure without overlapping or being cut off.\n    if label is None and predictions is None:                         # if there are no predictions and true labels     \n        plt.subplots_adjust(wspace=0, hspace=0)                       # no spacing between the images             \n    else:                                                             # otherwise\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)           # make spaces\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-18T17:53:09.495079Z","iopub.execute_input":"2023-09-18T17:53:09.495436Z","iopub.status.idle":"2023-09-18T17:53:09.527549Z","shell.execute_reply.started":"2023-09-18T17:53:09.495405Z","shell.execute_reply":"2023-09-18T17:53:09.526535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- You can display a single batch of images from a dataset with another of our helper functions. \n- The next cell will turn the dataset into an iterator of batches of 20 images.\n- Use the Python `next` function to pop out the next batch in the stream and display it with the helper function.\n- By defining `ds_iter` and `one_batch` in separate cells, you only need to rerun the second cell to see a new batch of images.","metadata":{}},{"cell_type":"code","source":"ds_iter = iter(ds_train.unbatch().batch(20))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:02:18.798192Z","iopub.execute_input":"2023-09-18T17:02:18.798723Z","iopub.status.idle":"2023-09-18T17:02:18.846921Z","shell.execute_reply.started":"2023-09-18T17:02:18.798691Z","shell.execute_reply":"2023-09-18T17:02:18.846029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_batch = next(ds_iter)\ndisplay_batch_of_images(one_batch)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:02:18.848204Z","iopub.execute_input":"2023-09-18T17:02:18.848622Z","iopub.status.idle":"2023-09-18T17:02:26.015301Z","shell.execute_reply.started":"2023-09-18T17:02:18.848597Z","shell.execute_reply":"2023-09-18T17:02:26.013933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Callbacks\n- A callback is a .fit() parameter, where you can pass different objects:\n- **`Learning Rate Schedule`**: adjusts the learning rate e.g. after a certain number of epochs or when the training loss plateaus or else\n- **`Early Stopping`**: stops learning if there is no improvement after several epochs\n- **`Checkpoint`**: saves weights at the end of every epoch, if it's the best seen so far during model.fit\n\n- There are [many other options](https://keras.io/api/callbacks/). But we will make these three here.\nWe will apply the same callbacks for all the models that follow.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 30                                           # EarlyStopping should break it sooner\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE   # Batches per epoch","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:53:09.534616Z","iopub.execute_input":"2023-09-18T17:53:09.537482Z","iopub.status.idle":"2023-09-18T17:53:09.543739Z","shell.execute_reply.started":"2023-09-18T17:53:09.537449Z","shell.execute_reply":"2023-09-18T17:53:09.542711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callback 1: learning rate schedule","metadata":{}},{"cell_type":"code","source":"def exponential_lr(epoch,                                   # The current training epoch\n                   start_lr = 0.00001,                      # The initial learning rate\n                   min_lr = 0.00001,                        # The minimum learning rate\n                   max_lr = 0.00005,                        # The maximum learning rate\n                   rampup_epochs = 5,                       # The number of epochs for a linear increase in learning rate\n                   sustain_epochs = 0,                      # The number of epochs to sustain the maximum learning rate\n                   exp_decay = 0.8):                        # The exponential decay factor for learning rate reduction\n\n    # calculates the learning rate for a given epoch based on the provided parameters\n    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay): \n        \n        if epoch < rampup_epochs:                             # For epochs less than rampup_epochs, the learning rate increases from start_lr to max_lr.\n            lr = ((max_lr - start_lr) /\n                  rampup_epochs * epoch + start_lr)\n        \n        elif epoch < rampup_epochs + sustain_epochs:          # From 'rampup_epochs' till 'rampup_epochs + sustain_epochs', the learning rate remains constant at max_lr\n            lr = max_lr\n        \n        else:                                                 # exponential decay towards min_lr\n            lr = ((max_lr - min_lr) *\n                  exp_decay**(epoch - rampup_epochs - sustain_epochs) +\n                  min_lr)\n        return lr\n    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n\n# This is what it was all about. We pass our customary funtion to the keras LearningRateScheduler to create a callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(exponential_lr, verbose=True)  \n\n# plot our customary learning rate per epoch\nrng = [i for i in range(EPOCHS)]     \ny = [exponential_lr(x) for x in rng]\nplt.plot(rng, y)\nprint(f'Learning rate schedule: \\n'\n      f'from {y[0]:.3g} \\n'\n      f'to {max(y):.3g} \\n'\n      f'and then back to {y[-1]:.3g}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-18T17:53:09.548914Z","iopub.execute_input":"2023-09-18T17:53:09.554744Z","iopub.status.idle":"2023-09-18T17:53:09.920274Z","shell.execute_reply.started":"2023-09-18T17:53:09.554700Z","shell.execute_reply":"2023-09-18T17:53:09.919281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callback 2: EarlyStopping\nIt will stop training when there is no improvement in the validation loss during the specified number of consecutive epochs. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.001,              # minimum change to count as an improvement\n    patience=5,                   # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:53:09.921581Z","iopub.execute_input":"2023-09-18T17:53:09.922620Z","iopub.status.idle":"2023-09-18T17:53:09.929063Z","shell.execute_reply.started":"2023-09-18T17:53:09.922583Z","shell.execute_reply":"2023-09-18T17:53:09.927883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callback 3. Checkpoint\n- Checkpoint allows recording the best performing configuration of a model's weights.\n- It also allows you to upload these weights to the model from a file instead of training it again.","metadata":{}},{"cell_type":"code","source":"Xception_checkpoint_filepath = 'Xception.h5'\n\nXception_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n                        filepath=Xception_checkpoint_filepath,\n                        save_weights_only=True,\n                        monitor='val_loss',\n                        mode='min',\n                        save_best_only=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:12:20.889443Z","iopub.execute_input":"2023-09-18T17:12:20.889832Z","iopub.status.idle":"2023-09-18T17:12:20.895383Z","shell.execute_reply.started":"2023-09-18T17:12:20.889801Z","shell.execute_reply":"2023-09-18T17:12:20.894320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Model_1 - Transfer learning\n\nNow we're ready to create a neural network for classifying images! \n- We'll use what's known as **transfer learning**: take a pretrained heavy model (base) and set your keras model on top of it (head)\n- We will use **Xception** as the base (cause it performs well on this dataset). Run the cell below to see the list of avalible bases in Keras\n- The distribution strategy we created earlier contains a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with), `strategy.scope`. When using a TPU, it's important to define your model in a strategy.scope() context.","metadata":{}},{"cell_type":"code","source":"# The list of avalible pretrained models (bases)\n', '.join(tf.keras.applications.__dir__())","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:12:24.050114Z","iopub.execute_input":"2023-09-18T17:12:24.051082Z","iopub.status.idle":"2023-09-18T17:12:24.059555Z","shell.execute_reply.started":"2023-09-18T17:12:24.051038Z","shell.execute_reply":"2023-09-18T17:12:24.058585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    pretrained_model = tf.keras.applications.Xception(\n                       weights='imagenet',                   \n                       include_top=False,                         # we will build our own head on top of this base, so we tell the the strategy to 'decapitate' the base\n                       input_shape=[*IMAGE_SIZE, 3]               # '*' unpacks the IMAGE_SIZE tuple, passing it's two elements as separate values\n    )\n    pretrained_model.trainable = False                            # transfer learning\n    \n    Xception = tf.keras.Sequential([                              # Here is our eventual model:\n        pretrained_model,                                         # add the pretrained base  \n        tf.keras.layers.GlobalAveragePooling2D(),                 # attach a new head (GlobalAveragePooling averages feature maps produced by the base down to a single value per feature. Which is just right for a classification)\n        tf.keras.layers.Dropout(0.3),                             # add regularization \n        tf.keras.layers.Dense(                                    # output layer where\n                            len(CLASSES),                         # number of neurons corresponds to the number of classes\n                            activation='softmax')                 # this is the activation function you want to use for a multi-class classification task\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:29:05.460227Z","iopub.execute_input":"2023-09-18T20:29:05.461195Z","iopub.status.idle":"2023-09-18T20:29:07.224839Z","shell.execute_reply.started":"2023-09-18T20:29:05.461160Z","shell.execute_reply":"2023-09-18T20:29:07.223851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xception.compile(\n    optimizer= 'nadam',                            # Nesterov-accelerated Adaptive Moment Estimation (nadam) is an extension of Adaptive Moment Estimation (adam)\n    loss     = 'sparse_categorical_crossentropy',  # The one you need for a multi-class classification\n    metrics  = ['sparse_categorical_accuracy'],    # The one you need for a multi-class classification\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:29:07.226835Z","iopub.execute_input":"2023-09-18T20:29:07.227228Z","iopub.status.idle":"2023-09-18T20:29:07.242972Z","shell.execute_reply.started":"2023-09-18T20:29:07.227196Z","shell.execute_reply":"2023-09-18T20:29:07.241819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xception_training = Xception.fit(\n                    ds_train,\n                    validation_data=ds_valid,\n                    epochs=EPOCHS,\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    callbacks=[lr_callback, early_stopping, Xception_checkpoint]    # Here is where our callbacks go\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:12:26.371519Z","iopub.execute_input":"2023-09-18T17:12:26.372450Z","iopub.status.idle":"2023-09-18T17:36:00.560574Z","shell.execute_reply.started":"2023-09-18T17:12:26.372416Z","shell.execute_reply":"2023-09-18T17:36:00.559499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xception_history_frame = pd.DataFrame(Xception_training.history)\nXception_history_frame.loc[:, ['loss', 'val_loss']].plot()\nXception_history_frame.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T17:45:56.647358Z","iopub.execute_input":"2023-09-18T17:45:56.647732Z","iopub.status.idle":"2023-09-18T17:45:57.405605Z","shell.execute_reply.started":"2023-09-18T17:45:56.647704Z","shell.execute_reply":"2023-09-18T17:45:57.404615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7: Model_2 - Custom CNN","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    custom_model = keras.Sequential([\n        # Preprocessing\n        preprocessing.RandomFlip(mode='horizontal'),  # meaning, left-to-right\n        preprocessing.RandomFlip(mode='vertical'),    # meaning, top-to-bottom\n        preprocessing.RandomRotation(factor=0.20),\n        preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n\n         # Block One\n        layers.BatchNormalization(renorm=True),\n        layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n        layers.MaxPool2D(),\n\n        # Block Two\n        layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n        layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n        layers.MaxPool2D(),\n\n        # Block Three\n        layers.BatchNormalization(renorm=True),\n        layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n        layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n        layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n        layers.MaxPool2D(),\n\n        # Head\n        layers.BatchNormalization(renorm=True),\n        layers.GlobalAveragePooling2D(),\n        layers.Dropout(0.1),\n        layers.Dense(len(CLASSES), activation='softmax') \n    ])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:29:25.062248Z","iopub.execute_input":"2023-09-18T20:29:25.063156Z","iopub.status.idle":"2023-09-18T20:29:25.102606Z","shell.execute_reply.started":"2023-09-18T20:29:25.063123Z","shell.execute_reply":"2023-09-18T20:29:25.101617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_model.compile(\n    optimizer='nadam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy'],\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:29:26.002262Z","iopub.execute_input":"2023-09-18T20:29:26.002944Z","iopub.status.idle":"2023-09-18T20:29:26.016791Z","shell.execute_reply.started":"2023-09-18T20:29:26.002911Z","shell.execute_reply":"2023-09-18T20:29:26.015773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checkpoint for the second model\ncustom_model_checkpoint_filepath = 'custom_model.h5'\n\ncustom_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n                        filepath=custom_model_checkpoint_filepath,\n                        save_weights_only=True,\n                        monitor='val_loss',\n                        mode='min',\n                        save_best_only=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:32:48.649812Z","iopub.execute_input":"2023-09-18T19:32:48.650172Z","iopub.status.idle":"2023-09-18T19:32:48.655644Z","shell.execute_reply.started":"2023-09-18T19:32:48.650143Z","shell.execute_reply":"2023-09-18T19:32:48.654482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_model_training = custom_model.fit(\n                ds_train,\n                validation_data=ds_valid,\n                epochs=EPOCHS,\n                steps_per_epoch=STEPS_PER_EPOCH,\n                callbacks=[lr_callback, early_stopping, custom_model_checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:32:54.375239Z","iopub.execute_input":"2023-09-18T19:32:54.375942Z","iopub.status.idle":"2023-09-18T20:15:22.496808Z","shell.execute_reply.started":"2023-09-18T19:32:54.375909Z","shell.execute_reply":"2023-09-18T20:15:22.495782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_frame = pd.DataFrame(custom_model_training.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:44:53.529977Z","iopub.execute_input":"2023-09-18T20:44:53.530427Z","iopub.status.idle":"2023-09-18T20:44:54.110247Z","shell.execute_reply.started":"2023-09-18T20:44:53.530393Z","shell.execute_reply":"2023-09-18T20:44:54.109290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7: Visual Validation\n- We will apply our **display_batch_of_images()** function to see the flowers, their predicted and true classes.\n- **Visual validation** can help reveal patterns of images the model has trouble with.","metadata":{}},{"cell_type":"code","source":"# Visual validation\ndataset = get_validation_dataset()\ndataset = dataset.unbatch().batch(20)     # Display 20 images at a time. Fill free to put your number\nbatch = iter(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:22:23.546252Z","iopub.execute_input":"2023-09-18T20:22:23.546833Z","iopub.status.idle":"2023-09-18T20:22:23.660578Z","shell.execute_reply.started":"2023-09-18T20:22:23.546801Z","shell.execute_reply":"2023-09-18T20:22:23.659568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Xception\n# Run the cell again to see another set\nimages, labels = next(batch)\nprobabilities = Xception.predict(images)\npredictions = np.argmax(probabilities, axis=-1)\ndisplay_batch_of_images((images, labels), predictions)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:31:57.747543Z","iopub.execute_input":"2023-09-18T20:31:57.747916Z","iopub.status.idle":"2023-09-18T20:32:01.119740Z","shell.execute_reply.started":"2023-09-18T20:31:57.747887Z","shell.execute_reply":"2023-09-18T20:32:01.118416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom_model\n# Run the cell again to see another set\nimages, labels = next(batch)\nprobabilities = custom_model.predict(images)\npredictions = np.argmax(probabilities, axis=-1)\ndisplay_batch_of_images((images, labels), predictions)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:32:25.022440Z","iopub.execute_input":"2023-09-18T20:32:25.022813Z","iopub.status.idle":"2023-09-18T20:32:27.978865Z","shell.execute_reply.started":"2023-09-18T20:32:25.022784Z","shell.execute_reply":"2023-09-18T20:32:27.977711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 9: Ensemble","metadata":{}},{"cell_type":"code","source":"cmdataset = get_validation_dataset(ordered=True)                               # since we are splitting the dataset and iterating separately on images and labels, order matters.\nimages_ds = cmdataset.map(lambda image, label: image)                          # makes a data set of images only\nlabels_ds = cmdataset.map(lambda image, label: label).unbatch()                # makes a data set of labels only\n\ncm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # gets everything as one batch in np.array format.\n\n# Xception.load_weights('/kaggle/input/petals-to-the-metal/Xception.h5')\n# custom_model.load_weights('/kaggle/working/custom_model.h5')\n\npredictions_1 = Xception.predict(images_ds)\npredictions_2 = custom_model.predict(images_ds)\n\nscores = []                                                                  # creat a list to store F1 scores for different alpha values. \nfor alpha in np.linspace(0,1,100):                                           # alpha is one of 100 evenly spaced values in range from 0 to 1\n    cm_probabilities = alpha * predictions_1 + (1-alpha) * predictions_2     # Combine predictions from two models using the alpha weight\n    cm_predictions = np.argmax(cm_probabilities, axis=-1)                    # For each example, select the class with the highest probability as the predicted class.\n    scores.append(f1_score(cm_correct_labels,                                # Calculate the F1 score between the correct labels \n                           cm_predictions,                                   # and the combined predictions.\n                           labels=range(len(CLASSES)),                       # It computes the F1 score for each class \n                           average='macro'))                                 # and returns the macro-average.\n\nprint(\"Correct labels: \",   cm_correct_labels.shape, cm_correct_labels)\nprint(\"Predicted labels: \", cm_predictions.shape,    cm_predictions)\nplt.plot(scores)\n\nbest_alpha = np.argmax(scores)/100                                           # 'scores' is a list of 100 avarage F1 values. We find the index of max F1 value. When we devide this index (e.g.35) by 100, we find our alpha, that produced that F1 index. Tricky, but elegant.\nprint (f'best_alpha: {best_alpha}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:33:14.544078Z","iopub.execute_input":"2023-09-18T20:33:14.544467Z","iopub.status.idle":"2023-09-18T20:33:41.798949Z","shell.execute_reply.started":"2023-09-18T20:33:14.544436Z","shell.execute_reply":"2023-09-18T20:33:41.797995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 10: Make Test Predictions","metadata":{}},{"cell_type":"code","source":"test_ds = get_test_dataset(ordered=True)\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\n\nm1 = Xception.predict(test_images_ds)\nm2 = custom_model.predict(test_images_ds)\n\nprobabilities = best_alpha*m1+(1-best_alpha)*m2\npredictions = np.argmax(probabilities, axis=-1)    # Find the class with the highest probability for each image:  \n                                                   # 'predictions' is a sequence of matrixes, where each matrix (idividual prediction) consists of 2 vectors: (1) all the classes (indexes) and (2) probabilities of an image to be that class\n                                                   # .argmax() returns index of a maximum value, i.e. class related to the highest probability value\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:41:21.978063Z","iopub.execute_input":"2023-09-18T20:41:21.978470Z","iopub.status.idle":"2023-09-18T20:42:03.034091Z","shell.execute_reply.started":"2023-09-18T20:41:21.978438Z","shell.execute_reply":"2023-09-18T20:42:03.033006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Generating submission.csv file...')\n\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()                # Get image ids from test set \ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')  # convert them to unicode\nsubmission_df = pd.DataFrame({'id': test_ids, 'label': predictions})\nsubmission_df.to_csv('submission.csv', index=False)\n\n\n# Look at the first few predictions\n!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2023-09-18T20:42:14.756995Z","iopub.execute_input":"2023-09-18T20:42:14.757383Z","iopub.status.idle":"2023-09-18T20:42:29.026835Z","shell.execute_reply.started":"2023-09-18T20:42:14.757349Z","shell.execute_reply":"2023-09-18T20:42:29.025581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 11: Make a submission \n\nIf you haven't already, create your own editable copy of this notebook by clicking on the **Copy and Edit** button in the top right corner. Then, submit to the competition by following these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.","metadata":{}},{"cell_type":"markdown","source":"# Step 12. Upvote the notebook\n\nI hope you enjoyed this journey, and as promised, it was indeed a pleasure. <br> If you've read this far please consider upvoting the notebook. Comments are welcome.","metadata":{}}]}